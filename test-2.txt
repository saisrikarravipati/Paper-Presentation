

# File: lib/collectors/sevenps/storage/postgres.py

from __future__ import annotations
import os
import threading
import asyncio
from typing import List, Dict, Optional

from psycopg import AsyncConnectionPool
from psycopg.errors import UniqueViolation
from lib.collectors.sevenps.base_storage import SevenPSStorage, BaseStorageException, RecordNotFoundException
from lib.logging.logger import get_logger
from lib.cyber_logging import get_cyber_logger
from lib.utils import generate_db_conn_info
from cyber_schema_model import Schema20Event  # <-- import corrected

logger = get_logger(__name__)
cyber_logger = get_cyber_logger()


class DatabaseConnectionError(BaseStorageException):
    """Cannot connect to DB"""


class DatabaseError(BaseStorageException):
    """General DB error"""


class PostgresDBStorage(SevenPSStorage):
    """
    A Postgres storage class whose public methods are all synchronous.
    Internally, this class spins up one background thread with its own asyncio loop,
    creates an AsyncConnectionPool (max_size=config["max_connections"]) on that loop,
    and all DB operations (execute, fetch_all, insert, etc.) happen via run_coroutine_threadsafe.
    """

    def __init__(self, config: Dict):
        """
        Synchronous constructor.  Spawns a background thread+loop, 
        then schedules _init_pool() on that loop.  
        Raises DatabaseConnectionError if pool creation fails.
        """
        self.config = config
        self._loop: asyncio.AbstractEventLoop
        self._thread: threading.Thread
        self._pool: Optional[AsyncConnectionPool] = None

        # 1) Build DSN
        is_cloud = os.getenv("CLOUD", "LOCAL").lower() == "true"
        if is_cloud:
            conninfo = generate_db_conn_info()
        else:
            # Local default for testing/DEV
            conninfo = (
                "host=localhost port=5432 dbname=testinginsightsdb "
                "user=user password=password"
            )

        # 2) Start a brand-new event loop in a separate daemon thread
        self._loop = asyncio.new_event_loop()
        self._thread = threading.Thread(
            target=self._start_background_loop,
            daemon=True,
            name="postgres-bg-loop",
        )
        self._thread.start()

        # 3) Schedule the actual pool‐creation coroutine on that loop.
        #    We block here until either pool is created or we get an exception.
        try:
            init_future = asyncio.run_coroutine_threadsafe(
                self._init_pool(conninfo), self._loop
            )
            # .result() either returns after pool is open, or raises if error
            init_future.result()
            logger.debug("Successfully initialized AsyncConnectionPool")
        except Exception as exc:
            # If pool creation fails, log an event and re‐raise as DatabaseConnectionError
            event = Schema20Event(
                event_name="Unable to connect to DB", 
                event_severity="ERROR"
            )
            cyber_logger.log(event)
            raise DatabaseConnectionError(
                "SEVENPS-COLLECTOR-ERROR: Unable to connect to DB"
            ) from exc

    def _start_background_loop(self):
        """
        Target for the background thread.  Just sets that thread's event loop,
        and calls run_forever().
        """
        asyncio.set_event_loop(self._loop)
        self._loop.run_forever()

    async def _init_pool(self, conninfo: str) -> None:
        """
        Actually create and open the AsyncConnectionPool on self._loop.
        We store it in self._pool (thread‐safe writes to self._pool).
        """
        min_c = self.config.get("min_connections", 1)
        max_c = self.config.get("max_connections", 5)
        self._pool = AsyncConnectionPool(
            conninfo=conninfo,
            min_size=min_c,
            max_size=max_c,
            timeout=180,
        )
        # Actually open all connections up front:
        await self._pool.open()
        logger.debug("Connection pool opened on background loop")

    async def _ensure_pool_open_async(self) -> None:
        """
        A private coroutine that ensures the pool exists
        and is not closed.  (If closed, we reopen it.)
        """
        if self._pool is None:
            raise DatabaseConnectionError("Pool was never initialized")
        if self._pool.closed:
            await self._pool.open()
            logger.debug("Re-opened connection pool")

    async def _close_pool_async(self) -> None:
        """
        Coroutine to close the pool cleanly.
        """
        if self._pool and not self._pool.closed:
            await self._pool.close()
            logger.debug("Connection pool closed")

    def close(self) -> None:
        """
        Synchronous close(): schedule _close_pool_async on the background loop,
        wait for it, then stop the loop and join the thread.
        """
        if not hasattr(self, "_loop"):
            return

        # 1) If pool exists, close it
        if self._pool is not None:
            close_future = asyncio.run_coroutine_threadsafe(
                self._close_pool_async(), self._loop
            )
            close_future.result()

        # 2) Stop the loop and join the thread
        self._loop.call_soon_threadsafe(self._loop.stop)
        self._thread.join(timeout=5)
        logger.debug("Background loop stopped, thread joined")

    #
    # ────────────────────────── Public "sync" methods ───────────────────────────
    #

    def _execute(self, query: str, params: Optional[Dict] = None) -> None:
        """
        Synchronous wrapper around _execute_async.  Blocks until done.
        """
        coro = self._execute_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()  # either returns None or raises

    def fetch_all(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        Synchronous wrapper around _fetch_all_async.  Returns List[tuple].
        """
        coro = self._fetch_all_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    def store_test_set_data(self, records: List[Dict]) -> None:
        """
        Synchronous wrapper around store_test_set_data_async.
        Raises ValueError if records is empty, or DatabaseError on repeated failure.
        """
        coro = self.store_test_set_data_async(records)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    def get_test_set_data(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        Synchronous wrapper around get_test_set_data_async.
        Returns list of JSON strings for results.
        """
        coro = self.get_test_set_data_async(test_request_id=test_request_id, artifact_id=artifact_id)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    #
    # ───────────────────────── Private "async" methods ──────────────────────────
    #

    async def _execute_async(self, query: str, params: Optional[Dict] = None) -> None:
        await self._ensure_pool_open_async()
        assert self._pool is not None  # for mypy
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    await conn.commit()
                    logger.info("Executed query: %s params: %s", query, params)
                except Exception as exc:
                    logger.error("Unable to execute query: %s", exc)
                    raise

    async def _fetch_all_async(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        await self._ensure_pool_open_async()
        assert self._pool is not None
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    rows = await cursor.fetchall()
                    return rows
                except Exception as exc:
                    logger.error("Unable to fetch from DB: %s", exc)
                    raise

    async def store_test_set_data_async(self, records: List[Dict]) -> None:
        if not records:
            logger.info("SEVENPS-COLLECTOR-ERROR: No records to store")
            raise ValueError("SEVENPS-COLLECTOR-ERROR: No records to store")

        tries = 6
        record_count = len(records)

        await self._ensure_pool_open_async()
        assert self._pool is not None

        while tries >= 0:
            try:
                async with self._pool.connection() as conn:
                    async with conn.cursor() as cursor:
                        for record in records:
                            logger.info(
                                "Inserting record - Artifact: %s:%s",
                                record.get("artifact_url"),
                                record.get("artifact_version")
                            )
                            await cursor.execute(
                                """
                                INSERT INTO sevenps_result_set
                                    (repo_url, artifact_url, artifact_name, artifact_version,
                                     test_set_type, test_request_id, component_asv, component_bap,
                                     report_doc, report_source, traceability_doc,
                                     github_org, github_repo, github_branch,
                                     pr_url, pr_id_branch, pr_source_branch,
                                     build_id, test_type_details, test_run_status)
                                VALUES
                                    (%(repo_url)s, %(artifact_url)s, %(artifact_name)s, %(artifact_version)s,
                                     %(test_set_type)s, %(test_request_id)s, %(component_asv)s, %(component_bap)s,
                                     %(report_doc)s, %(report_source)s, %(traceability_doc)s,
                                     %(github_org)s, %(github_repo)s, %(github_branch)s,
                                     %(pr_url)s, %(pr_id_branch)s, %(pr_source_branch)s,
                                     %(build_id)s, %(test_type_details)s, %(test_run_status)s)
                                """,
                                {
                                    "repo_url": record.get("repo_url"),
                                    "artifact_url": record.get("artifact_url"),
                                    "artifact_name": record.get("artifact_name"),
                                    "artifact_version": record.get("artifact_version"),
                                    "test_set_type": record.get("test_set_type"),
                                    "test_request_id": record.get("test_request_id"),
                                    "component_asv": record.get("component_asv"),
                                    "component_bap": record.get("component_bap"),
                                    "report_doc": record.get("report_doc"),
                                    "report_source": record.get("report_source"),
                                    "traceability_doc": record.get("traceability_doc"),
                                    "github_org": record.get("github_org"),
                                    "github_repo": record.get("github_repo"),
                                    "github_branch": record.get("github_branch"),
                                    "pr_url": record.get("pr_url"),
                                    "pr_id_branch": record.get("pr_id_branch"),
                                    "pr_source_branch": record.get("pr_source_branch"),
                                    "build_id": record.get("build_id"),
                                    "test_type_details": record.get("test_type_details"),
                                    "test_run_status": record.get("test_run_status"),
                                }
                            )
                        await conn.commit()
                        logger.info(
                            "Successfully wrote %d records to sevenps_result_set", record_count
                        )
                        return  # done
            except UniqueViolation as uv:
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Unique Violation when writing to DB :: %s", str(uv)
                )
                raise
            except Exception as exc:
                tries -= 1
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Couldn't write record to DB, tries left: %d :: %s",
                    tries,
                    str(exc),
                )
                await asyncio.sleep(30)
                if tries < 0:
                    event = Schema20Event(
                        event_name="Unable to write to DB",
                        event_severity="ERROR",
                        event_outcome="FAILURE",
                    )
                    cyber_logger.log(event)
                    logger.error("SEVENPS-COLLECTOR-ERROR: Bad Record: %s", str(record))
                    raise DatabaseError(
                        "SEVENPS-COLLECTOR-ERROR: Unable to write record to db: "
                        + str(exc)
                    ) from exc

    async def get_test_set_data_async(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        Pick exactly one filter: artifact_id or test_request_id.
        Return a list of JSON strings from the `results` column.
        """
        if artifact_id:
            sql = "SELECT results FROM sevenps_result_set WHERE artifact_id = $1"
            params = (artifact_id,)
        elif test_request_id:
            sql = "SELECT results FROM sevenps_result_set WHERE test_request_id = $1"
            params = (test_request_id,)
        else:
            raise ValueError("artifact_id or test_request_id is required")

        await self._ensure_pool_open_async()
        assert self._pool is not None

        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(sql, params)
                rows = await cursor.fetchall()

        if not rows:
            raise RecordNotFoundException(
                "SEVENPS-COLLECTOR-ERROR: No test results found with the provided ID"
            )
        return [r[0] for r in rows]


--------------------------

# File: tests/test_postgres.py

import unittest
import importlib
import os
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch
from psycopg import errors

from lib.collectors.sevenps.storage.postgres import (
    PostgresDBStorage,
    DatabaseConnectionError,
    DatabaseError,
    RecordNotFoundException,
)

PATCH_ROOT = "lib.collectors.sevenps.storage.postgres"


def _make_pool(
    closed: bool = True
):
    """
    Return (fake_pool, fake_cursor) pair.  
    - fake_pool is an AsyncMock that mimics AsyncConnectionPool
    - fake_cursor is the leaf AsyncMock that records .execute(...) and .fetchall()
    """
    fake_pool = AsyncMock()
    fake_pool.closed = closed
    fake_pool.open = AsyncMock()
    fake_pool.close = AsyncMock()

    # Create a fake connection object that has .commit() as AsyncMock
    fake_conn = AsyncMock()
    fake_conn.commit = AsyncMock()

    # Create a fake cursor object
    fake_cursor = AsyncMock()
    fake_cursor.execute = AsyncMock()
    fake_cursor.fetchall = AsyncMock()

    # Make sure `conn.cursor()` → async context manager that yields fake_cursor
    fake_cur_cm = AsyncMock()
    fake_cur_cm.__aenter__.return_value = fake_cursor
    fake_cur_cm.__aexit__.return_value = False
    fake_conn.cursor.return_value = fake_cur_cm

    # Make sure `pool.connection()` → async context manager that yields fake_conn
    fake_conn_cm = AsyncMock()
    fake_conn_cm.__aenter__.return_value = fake_conn
    fake_conn_cm.__aexit__.return_value = False
    fake_pool.connection.return_value = fake_conn_cm

    return fake_pool, fake_cursor


class TestPostgresDBStorage(unittest.TestCase):
    @patch(f"{PATCH_ROOT}.AsyncConnectionPool")
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    @patch(f"{PATCH_ROOT}.os.getenv", return_value="LOCAL")
    def test_init_local_success(self, m_env, m_run_tc, m_pool_cls):
        """
        Ensure that __init__ calls AsyncConnectionPool(...) once,
        and that it tries to open the pool by scheduling _init_pool().
        """
        # 1) Make AsyncConnectionPool(...) return a dummy pool
        dummy_pool, _ = _make_pool(closed=True)
        m_pool_cls.return_value = dummy_pool

        # 2) Make run_coroutine_threadsafe(...) return a Dummy future whose .result() does nothing
        def fake_run(coro, loop):
            class _DummyFuture:
                def result(self_inner):
                    # If _init_pool(...) tries to open, just return None
                    return None
            return _DummyFuture()

        m_run_tc.side_effect = fake_run

        # 3) Call constructor.  No exception should be raised.
        storage = PostgresDBStorage({"min_connections": 1, "max_connections": 2})

        # 4) Confirm AsyncConnectionPool was called with correct args (local mode)
        m_pool_cls.assert_called_once()
        # 5) Confirm run_coroutine_threadsafe was invoked once for _init_pool
        self.assertTrue(m_run_tc.called)

    @patch(f"{PATCH_ROOT}.AsyncConnectionPool", side_effect=Exception("boom"))
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    @patch(f"{PATCH_ROOT}.os.getenv", return_value="CLOUD")
    def test_init_cloud_failure(self, m_env, m_run_tc, m_pool_cls):
        """
        If AsyncConnectionPool(...) throws, __init__ should raise DatabaseConnectionError.
        """
        # __init__ will do:
        #   pool = AsyncConnectionPool(...)  <-- exception right here
        #   (no pool means we never schedule run_coroutine_threadsafe)
        with self.assertRaises(DatabaseConnectionError):
            PostgresDBStorage({})

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_close_closes_pool_and_stops_loop(self, m_run_tc):
        """
        After initialization, calling close() should schedule _close_pool_async()
        and then stop the background thread/loop.  
        We patch run_coroutine_threadsafe so that .result() returns immediately.
        """
        # 1) Create a dummy PostgresDBStorage but bypass full init logic
        #    We'll monkey-patch the instance so that it already has a loop and pool.
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._loop = asyncio.get_event_loop()
        # Make a fake pool that is open
        fake_pool, _ = _make_pool(closed=False)
        storage._pool = fake_pool
        storage._thread = MagicMock()

        # 2) Patch run_coroutine_threadsafe to return a dummy future.
        def fake_run(coro, loop):
            class _F:
                def result(self_inner):
                    return None
            return _F()
        m_run_tc.side_effect = fake_run

        # 3) Call close().  No exceptions.
        storage.close()

        # 4) It should have scheduled _close_pool_async
        self.assertTrue(m_run_tc.called)
        # 5) Finally, it should have asked to stop the loop
        #    (call_soon_threadsafe is not patched, but at least we didn't crash)
        #    We check that storage._thread.join(...) is called
        storage._thread.join.assert_called()

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_execute_success(self, m_run_tc):
        """
        Test the synchronous _execute(...) wrapper:
        - Patch run_coroutine_threadsafe so that .result() runs the coroutine immediately.
        - The underlying coroutine should call cursor.execute(...) and conn.commit().
        """
        # 1) Build a fake pool & cursor.  We'll attach them to storage._pool via the loop‐thread logic.
        fake_pool, fake_cursor = _make_pool(closed=False)

        # 2) Monkey‐patch the instance so that pool is already set:
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.get_event_loop()  # run coros on the current loop
        # We don't actually start a thread, because we will override run_coroutine_threadsafe.

        # 3) Patch run_coroutine_threadsafe to run the coroutine immediately
        def fake_run(coro, loop):
            class _F:
                def result(self_inner):
                    # Actually run the coro on the current loop:
                    return asyncio.get_event_loop().run_until_complete(coro)
            return _F()

        m_run_tc.side_effect = fake_run

        # 4) Call storage._execute(...)
        storage._execute("INSERT INTO xyz(id) VALUES(%s)", {"id": 1})

        # 5) Confirm that cursor.execute(...) was indeed awaited once (inside the coroutine)
        fake_cursor.execute.assert_awaited_once_with("INSERT INTO xyz(id) VALUES(%s)", {"id": 1})
        # 6) Confirm that conn.commit() was awaited once
        fake_pool.connection.return_value.__aenter__.return_value.commit.assert_awaited_once()

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_execute_failure_raises(self, m_run_tc):
        """
        If cursor.execute(...) inside the coroutine raises, _execute(...) should re-raise.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        # Make fake_cursor.execute raise
        fake_cursor.execute.side_effect = Exception("boom")

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.get_event_loop()

        def fake_run(coro, loop):
            class _F:
                def result(self_inner):
                    return asyncio.get_event_loop().run_until_complete(coro)
            return _F()
        m_run_tc.side_effect = fake_run

        with self.assertRaises(Exception):
            storage._execute("INSERT bad", None)

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_fetch_all(self, m_run_tc):
        """
        Test that fetch_all(...) returns whatever the coroutine returned.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        # Make the cursor.fetchall() return some rows
        fake_cursor.fetchall.return_value = asyncio.get_event_loop().create_task(asyncio.sleep(0, result=[(1,), (2,)]))

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.get_event_loop()

        def fake_run(coro, loop):
            class _F:
                def result(self_inner):
                    return asyncio.get_event_loop().run_until_complete(coro)
            return _F()
        m_run_tc.side_effect = fake_run

        rows = storage.fetch_all("SELECT * FROM foo", None)
        self.assertEqual(rows, [(1,), (2,)])

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_store_test_set_data_value_error(self, m_run_tc):
        """
        If records == [], the synchronous wrapper should immediately raise ValueError.
        It shouldn’t even try to schedule anything.
        """
        fake_pool, _ = _make_pool(closed=False)
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.get_event_loop()

        # In this case, store_test_set_data should never call run_coroutine_threadsafe
        storage._execute = PostgresDBStorage._execute  # no-op
        with self.assertRaises(ValueError):
            storage.store_test_set_data([])

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_store_test_set_data_success(self, m_run_tc):
        """
        If you supply one well‐formed record, no exception should be raised.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.get_event_loop()

        def fake_run(coro, loop):
            class _F:
                def result(self_inner):
                    return asyncio.get_event_loop().run_until_complete(coro)
            return _F()

        m_run_tc.side_effect = fake_run

        # Build a dummy record with every required key
        record = {k: "x" for k in (
            "repo_url", "artifact_url", "artifact_name", "artifact_version",
            "test_set_type", "test_request_id", "component_asv", "component_bap",
            "report_doc", "report_source", "traceability_doc",
            "github_org", "github_repo", "github_branch",
            "pr_url", "pr_id_branch", "pr_source_branch",
            "build_id", "test_type_details", "test_run_status"
        )}
        # Should not raise
        storage.store_test_set_data([record])

        # Ensure at least one cursor.execute(...) was called inside the coroutine
        self.assertTrue(fake_cursor.execute.await_count >= 1)

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_get_test_set_data_artifact_success(self, m_run_tc):
        """
        If get_test_set_data(...) finds a row, it should return a list of strings.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        # Make cursor.fetchall() return two JSON strings
        fake_cursor.fetchall.return_value = [("json-1",), ("json-2",)]

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.get_event_loop()

        def fake_run(coro, loop):
            class _F:
                def result(self_inner):
                    return asyncio.get_event_loop().run_until_complete(coro)
            return _F()

        m_run_tc.side_effect = fake_run

        out = storage.get_test_set_data(artifact_id="A-1")
        self.assertEqual(out, ["json-1", "json-2"])

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_get_test_set_data_not_found(self, m_run_tc):
        """
        If no rows are returned by fetchall(), get_test_set_data(...) should raise RecordNotFoundException.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        fake_cursor.fetchall.return_value = []

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.get_event_loop()

        def fake_run(coro, loop):
            class _F:
                def result(self_inner):
                    return asyncio.get_event_loop().run_until_complete(coro)
            return _F()

        m_run_tc.side_effect = fake_run

        with self.assertRaises(RecordNotFoundException):
            storage.get_test_set_data(test_request_id="T-1")


if __name__ == "__main__":
    unittest.main()



