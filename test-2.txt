
# File: lib/collectors/sevenps/storage/postgres.py

from __future__ import annotations
import os
import threading
import asyncio
from typing import List, Dict, Optional

from psycopg import AsyncConnectionPool
from psycopg.errors import UniqueViolation
from lib.collectors.sevenps.base_storage import SevenPSStorage, BaseStorageException, RecordNotFoundException
from lib.logging.logger import get_logger
from lib.cyber_logging import get_cyber_logger
from lib.utils import generate_db_conn_info
from cyber_schema_model import Schema20Event  # ← make sure this path matches your project

logger = get_logger(__name__)
cyber_logger = get_cyber_logger()


class DatabaseConnectionError(BaseStorageException):
    """Cannot connect to DB"""


class DatabaseError(BaseStorageException):
    """General DB error"""


class PostgresDBStorage(SevenPSStorage):
    """
    A Postgres storage class whose public methods are all synchronous (or
    coroutine‐compatible).  Internally, it spins up one background thread +
    a dedicated asyncio loop, builds an AsyncConnectionPool there, and all
    DB work is scheduled via `asyncio.run_coroutine_threadsafe(...)`.
    """

    def __init__(self, config: Dict):
        """
        Synchronous constructor.  Spawns a background thread+loop, then
        schedules _init_pool(conninfo) on that loop.  Raises
        DatabaseConnectionError if pool creation fails.
        """
        self.config = config
        self._loop: asyncio.AbstractEventLoop
        self._thread: threading.Thread
        self._pool: Optional[AsyncConnectionPool] = None

        # 1) Determine mode from env:
        mode = os.getenv("CLOUD", "LOCAL").upper()
        is_cloud = (mode == "CLOUD")

        if is_cloud:
            conninfo = generate_db_conn_info()
        else:
            # Local default for testing/DEV
            conninfo = (
                "host=localhost port=5432 dbname=testinginsightsdb "
                "user=user password=password"
            )

        # 2) Start a brand-new event loop in a separate daemon thread
        self._loop = asyncio.new_event_loop()
        self._thread = threading.Thread(
            target=self._start_background_loop,
            daemon=True,
            name="postgres-bg-loop",
        )
        self._thread.start()

        # 3) Schedule the actual pool‐creation coroutine on that loop.
        try:
            init_future = asyncio.run_coroutine_threadsafe(
                self._init_pool(conninfo), self._loop
            )
            # .result() will raise if pool creation failed
            init_future.result()
            logger.debug("Successfully initialized AsyncConnectionPool")
        except Exception as exc:
            # If pool creation fails, log an event and re‐raise as DatabaseConnectionError
            event = Schema20Event(
                event_name="Unable to connect to DB",
                event_severity="ERROR"
            )
            cyber_logger.log(event)
            raise DatabaseConnectionError(
                "SEVENPS-COLLECTOR-ERROR: Unable to connect to DB"
            ) from exc

    def __del__(self):
        """
        If someone forgets to call `close()`, ensure we stop our background loop.
        """
        try:
            if hasattr(self, "_loop") and self._loop.is_running():
                asyncio.run_coroutine_threadsafe(self._close_pool(), self._loop).result()
                self._loop.call_soon_threadsafe(self._loop.stop)
                self._thread.join(timeout=1)
        except Exception:
            pass

    def _start_background_loop(self):
        """
        Target for the background thread.  Just sets that thread's event loop,
        and calls run_forever().
        """
        asyncio.set_event_loop(self._loop)
        self._loop.run_forever()

    async def _init_pool(self, conninfo: str) -> None:
        """
        Actually create and open the AsyncConnectionPool on self._loop.
        We store it in self._pool (thread‐safe).
        """
        min_c = self.config.get("min_connections", 1)
        max_c = self.config.get("max_connections", 5)
        self._pool = AsyncConnectionPool(
            conninfo=conninfo,
            min_size=min_c,
            max_size=max_c,
            timeout=180,
        )
        # Actually open all connections up front:
        await self._pool.open()
        logger.debug("Connection pool opened on background loop")

    async def _ensure_pool_open(self) -> None:
        """
        A private coroutine that ensures the pool exists and is not closed.
        (If closed, we reopen it.)
        """
        if self._pool is None:
            raise DatabaseConnectionError("Pool was never initialized")
        if self._pool.closed:
            await self._pool.open()
            logger.debug("Re-opened connection pool")

    async def _close_pool(self) -> None:
        """
        Coroutine to close the pool cleanly.
        """
        if self._pool and not self._pool.closed:
            await self._pool.close()
            logger.debug("Connection pool closed")

    def close(self) -> None:
        """
        Synchronous close(): schedule _close_pool on the background loop,
        wait for it, then stop the loop and join the thread.
        """
        if not hasattr(self, "_loop"):
            return

        # 1) If pool exists, close it
        if self._pool is not None:
            close_future = asyncio.run_coroutine_threadsafe(
                self._close_pool(), self._loop
            )
            close_future.result()

        # 2) Stop the loop and join the thread
        self._loop.call_soon_threadsafe(self._loop.stop)
        self._thread.join(timeout=5)
        logger.debug("Background loop stopped, thread joined")

    #
    # ───────────────────── Public “sync” methods ──────────────────────────
    #

    def _execute(self, query: str, params: Optional[Dict] = None) -> None:
        """
        Synchronous wrapper around _execute_async.  Blocks until done.
        (Tests that do `await storage._execute(...)` will continue to work, too,
         because Python allows awaiting a coroutine that returns None.)
        """
        coro = self._execute_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def _execute_async(self, query: str, params: Optional[Dict] = None) -> None:
        """
        The “real” coroutine that does an async cursor.execute + commit.
        """
        await self._ensure_pool_open()
        assert self._pool is not None  # for type‐checkers
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    await conn.commit()
                    logger.info("Executed query: %s params: %s", query, params)
                except Exception as exc:
                    logger.error("Unable to execute query: %s", exc)
                    raise

    def fetch_all(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        Synchronous wrapper around fetch_all_async.  Returns List[tuple].
        """
        coro = self.fetch_all_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def fetch_all_async(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        The “real” coroutine that does an async cursor.execute + fetchall.
        """
        await self._ensure_pool_open()
        assert self._pool is not None
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    rows = await cursor.fetchall()
                    return rows
                except Exception as exc:
                    logger.error("Unable to fetch from DB: %s", exc)
                    raise

    def store_test_set_data(self, records: List[Dict]) -> None:
        """
        Synchronous wrapper around store_test_set_data_async.
        Raises ValueError if records is empty, or DatabaseError on repeated failure.
        """
        coro = self.store_test_set_data_async(records)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def store_test_set_data_async(self, records: List[Dict]) -> None:
        """
        The real coroutine: inserts each record into sevenps_result_set, with retries.
        """
        if not records:
            logger.info("SEVENPS-COLLECTOR-ERROR: No records to store")
            raise ValueError("SEVENPS-COLLECTOR-ERROR: No records to store")

        tries = 6
        record_count = len(records)

        await self._ensure_pool_open()
        assert self._pool is not None

        while tries >= 0:
            try:
                async with self._pool.connection() as conn:
                    async with conn.cursor() as cursor:
                        for record in records:
                            logger.info(
                                "Inserting record – Artifact: %s:%s",
                                record.get("artifact_url"),
                                record.get("artifact_version"),
                            )
                            await cursor.execute(
                                """
                                INSERT INTO sevenps_result_set
                                    (repo_url, artifact_url, artifact_name, artifact_version,
                                     test_set_type, test_request_id, component_asv, component_bap,
                                     report_doc, report_source, traceability_doc,
                                     github_org, github_repo, github_branch,
                                     pr_url, pr_id_branch, pr_source_branch,
                                     build_id, test_type_details, test_run_status)
                                VALUES
                                    (%(repo_url)s, %(artifact_url)s, %(artifact_name)s, %(artifact_version)s,
                                     %(test_set_type)s, %(test_request_id)s, %(component_asv)s, %(component_bap)s,
                                     %(report_doc)s, %(report_source)s, %(traceability_doc)s,
                                     %(github_org)s, %(github_repo)s, %(github_branch)s,
                                     %(pr_url)s, %(pr_id_branch)s, %(pr_source_branch)s,
                                     %(build_id)s, %(test_type_details)s, %(test_run_status)s)
                                """,
                                {
                                    "repo_url": record.get("repo_url"),
                                    "artifact_url": record.get("artifact_url"),
                                    "artifact_name": record.get("artifact_name"),
                                    "artifact_version": record.get("artifact_version"),
                                    "test_set_type": record.get("test_set_type"),
                                    "test_request_id": record.get("test_request_id"),
                                    "component_asv": record.get("component_asv"),
                                    "component_bap": record.get("component_bap"),
                                    "report_doc": record.get("report_doc"),
                                    "report_source": record.get("report_source"),
                                    "traceability_doc": record.get("traceability_doc"),
                                    "github_org": record.get("github_org"),
                                    "github_repo": record.get("github_repo"),
                                    "github_branch": record.get("github_branch"),
                                    "pr_url": record.get("pr_url"),
                                    "pr_id_branch": record.get("pr_id_branch"),
                                    "pr_source_branch": record.get("pr_source_branch"),
                                    "build_id": record.get("build_id"),
                                    "test_type_details": record.get("test_type_details"),
                                    "test_run_status": record.get("test_run_status"),
                                },
                            )
                        await conn.commit()
                        logger.info(
                            "Successfully wrote %d records to sevenps_result_set", record_count
                        )
                        return  # done!
            except UniqueViolation as uv:
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Unique Violation when writing to DB :: %s",
                    str(uv),
                )
                raise
            except Exception as exc:
                tries -= 1
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Couldn't write record to DB, tries left: %d :: %s",
                    tries,
                    str(exc),
                )
                # Sleep before retrying
                await asyncio.sleep(30)
                if tries < 0:
                    event = Schema20Event(
                        event_name="Unable to write to DB",
                        event_severity="ERROR",
                        event_outcome="FAILURE",
                    )
                    cyber_logger.log(event)
                    logger.error("SEVENPS-COLLECTOR-ERROR: Bad Record: %s", str(record))
                    raise DatabaseError(
                        "SEVENPS-COLLECTOR-ERROR: Unable to write record to db: " + str(exc)
                    ) from exc

    def get_test_set_data(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        Synchronous wrapper around get_test_set_data_async.
        Returns a list of JSON strings.
        """
        coro = self.get_test_set_data_async(test_request_id=test_request_id, artifact_id=artifact_id)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def get_test_set_data_async(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        The real coroutine: SELECT results FROM sevenps_result_set WHERE [filter].
        Returns a list of JSON strings (first column of each row).
        """
        if artifact_id:
            sql = "SELECT results FROM sevenps_result_set WHERE artifact_id = $1"
            params = (artifact_id,)
        elif test_request_id:
            sql = "SELECT results FROM sevenps_result_set WHERE test_request_id = $1"
            params = (test_request_id,)
        else:
            raise ValueError("artifact_id or test_request_id is required")

        await self._ensure_pool_open()
        assert self._pool is not None

        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(sql, params)
                rows = await cursor.fetchall()

        if not rows:
            raise RecordNotFoundException(
                "SEVENPS-COLLECTOR-ERROR: No test results found with the provided ID"
            )
        return [r[0] for r in rows]



----------------------------


# File: tests/test_db.py

import unittest
import importlib
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch
from psycopg import errors

from lib.collectors.sevenps.storage.postgres import (
    PostgresDBStorage,
    DatabaseConnectionError,
    DatabaseError,
    RecordNotFoundException,
)

PATCH_ROOT = "lib.collectors.sevenps.storage.postgres"


def _make_pool(closed: bool = True):
    """
    Return a tuple (fake_pool, fake_cursor), where:
      - fake_pool is an AsyncMock mimicking AsyncConnectionPool
      - fake_cursor is the AsyncMock used for all cursor.execute()/fetchall() calls
    """
    fake_pool = AsyncMock()
    fake_pool.closed = closed
    fake_pool.open = AsyncMock()
    fake_pool.close = AsyncMock()

    # Create a fake connection object with commit() as AsyncMock
    fake_conn = AsyncMock()
    fake_conn.commit = AsyncMock()

    # Create a fake cursor object
    fake_cursor = AsyncMock()
    fake_cursor.execute = AsyncMock()
    fake_cursor.fetchall = AsyncMock()

    # Make conn.cursor() → async context manager that yields fake_cursor
    fake_cur_cm = AsyncMock()
    fake_cur_cm.__aenter__.return_value = fake_cursor
    fake_cur_cm.__aexit__.return_value = False
    fake_conn.cursor.return_value = fake_cur_cm

    # Make pool.connection() → async context manager that yields fake_conn
    fake_conn_cm = AsyncMock()
    fake_conn_cm.__aenter__.return_value = fake_conn
    fake_conn_cm.__aexit__.return_value = False
    fake_pool.connection.return_value = fake_conn_cm

    return fake_pool, fake_cursor


class TestPostgresDBStorage(unittest.TestCase):
    # ───────────────────────── constructor paths ────────────────────────────── #
    @patch(f"{PATCH_ROOT}.AsyncConnectionPool")
    @patch(f"{PATCH_ROOT}.os.getenv", return_value="LOCAL")
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_init_local_success(self, m_run_tc, m_env, m_pool_cls):
        """
        When os.getenv(...) returns "LOCAL", we should create the pool with the local DSN,
        and run_coroutine_threadsafe must be called once (to schedule _init_pool).
        """
        # 1) Set AsyncConnectionPool(...) → dummy_pool
        dummy_pool, _ = _make_pool(closed=True)
        m_pool_cls.return_value = dummy_pool

        # 2) Make run_coroutine_threadsafe(...) return an object whose .result() does nothing
        class DummyFuture:
            def result(self):
                return None

        m_run_tc.return_value = DummyFuture()

        # 3) Construct—no exception should be raised
        PostgresDBStorage({"min_connections": 1, "max_connections": 2})

        # 4) Confirm AsyncConnectionPool was called exactly once
        m_pool_cls.assert_called_once()
        # 5) Confirm run_coroutine_threadsafe was invoked once (for _init_pool)
        self.assertTrue(m_run_tc.called)

    @patch(f"{PATCH_ROOT}.AsyncConnectionPool", side_effect=Exception("boom"))
    @patch(f"{PATCH_ROOT}.os.getenv", return_value="CLOUD")
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_init_cloud_failure(self, m_run_tc, m_env, m_pool_cls):
        """
        When os.getenv(...) returns "CLOUD" and AsyncConnectionPool(...) raises,
        __init__ should raise DatabaseConnectionError.
        """
        with self.assertRaises(DatabaseConnectionError):
            PostgresDBStorage({})

    # ─────────────── ensure_pool_open() + close() behaviour ─────────────────── #
    def test_pool_open_and_close(self):
        """
        - If pool.closed=True, await pool.open() exactly once.
        - If pool.closed=False, do not open again.
        - _close_pool() should await pool.close().
        """
        pool, _ = _make_pool(closed=True)
        storage = type("Dummy", (), {})()
        storage._pool = pool

        # Bind the coroutine method onto our dummy storage
        ensure = PostgresDBStorage._ensure_pool_open.__get__(storage)
        close_coro = PostgresDBStorage._close_pool.__get__(storage)

        # pool.closed=True → open() should run once
        asyncio.run(ensure())
        pool.open.assert_awaited_once()

        # Already open → no second open()
        pool.closed = False
        asyncio.run(ensure())
        pool.open.assert_awaited_once()

        # Now close() should run pool.close()
        asyncio.run(close_coro())
        pool.close.assert_awaited_once()

    # ───────────────────────── _execute() paths ─────────────────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_execute_success(self, m_run_tc):
        """
        Test the synchronous _execute(...) wrapper:
        - Patch run_coroutine_threadsafe so that .result() runs _execute_async immediately.
        - The underlying coroutine should call cursor.execute(...) and conn.commit().
        """
        fake_pool, fake_cursor = _make_pool(closed=False)

        # Make a dummy storage with pool already set
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()  # unused, because we patch run_coroutine_threadsafe

        # Override run_coroutine_threadsafe to run the coroutine via asyncio.run
        def fake_run(coro, loop):
            # Immediately execute the coroutine on a fresh loop
            return asyncio.get_event_loop().run_until_complete(coro)

        class DummyFuture:
            def __init__(self, result):
                self._result = result

            def result(self):
                return self._result

        # We want run_coroutine_threadsafe(...) → DummyFuture( None ),
        # but also cause the coroutine to run. So:
        def fake_run_return(coro, loop):
            res = asyncio.get_event_loop().run_until_complete(coro)
            return DummyFuture(res)

        m_run_tc.side_effect = fake_run_return

        # Call _execute – it should not raise
        storage._execute("INSERT INTO xyz(id) VALUES(%s)", {"id": 1})

        # Confirm that cursor.execute(...) was awaited once inside _execute_async
        fake_cursor.execute.assert_awaited_once_with(
            "INSERT INTO xyz(id) VALUES(%s)", {"id": 1}
        )
        # Confirm that conn.commit() was awaited once
        fake_pool.connection.return_value.__aenter__.return_value.commit.assert_awaited_once()

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_execute_failure(self, m_run_tc):
        """
        If cursor.execute(...) inside _execute_async() raises, _execute(...) should re‐raise.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        fake_cursor.execute.side_effect = Exception("boom")

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        def fake_run_return(coro, loop):
            return asyncio.get_event_loop().run_until_complete(coro)

        class DummyFuture:
            def __init__(self, result_or_exc):
                self._val = result_or_exc

            def result(self):
                # If result is an exception, re‐raise it
                if isinstance(self._val, Exception):
                    raise self._val
                return self._val

        # We cause the coroutine to run, capturing any exception
        def fake_run(coro, loop):
            try:
                res = asyncio.get_event_loop().run_until_complete(coro)
                return DummyFuture(res)
            except Exception as e:
                return DummyFuture(e)

        m_run_tc.side_effect = fake_run

        with self.assertRaises(Exception):
            storage._execute("INSERT BAD", None)

    # ───────────────────────── _fetch_all() path ───────────────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_fetch_all(self, m_run_tc):
        """
        Test that fetch_all(...) returns whatever fetch_all_async returned.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        # Make fake_cursor.fetchall() return a list of tuples
        fake_cursor.fetchall.return_value = [(1,), (2,)]

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, val):
                self._val = val

            def result(self):
                return self._val

        def fake_run(coro, loop):
            res = asyncio.get_event_loop().run_until_complete(coro)
            return DummyFuture(res)

        m_run_tc.side_effect = fake_run

        rows = storage.fetch_all("SELECT * FROM foo", None)
        self.assertEqual(rows, [(1,), (2,)])

    # ───────────── store_test_set_data() edge + happy paths ────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_store_test_set_data_value_error(self, m_run_tc):
        """
        If records == [], synchronous wrapper must raise ValueError immediately.
        """
        fake_pool, _ = _make_pool(closed=False)
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        # Because we pass [], it should never even call run_coroutine_threadsafe:
        with self.assertRaises(ValueError):
            storage.store_test_set_data([])

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_store_test_set_data_success(self, m_run_tc):
        """
        If you supply a well‐formed record, no exception should be raised,
        and at least one cursor.execute(...) is awaited.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, val):
                self._val = val

            def result(self):
                return self._val

        def fake_run(coro, loop):
            res = asyncio.get_event_loop().run_until_complete(coro)
            return DummyFuture(res)

        m_run_tc.side_effect = fake_run

        # Build one dummy record with every required key
        record = {k: "x" for k in (
            "repo_url", "artifact_url", "artifact_name", "artifact_version",
            "test_set_type", "test_request_id", "component_asv", "component_bap",
            "report_doc", "report_source", "traceability_doc",
            "github_org", "github_repo", "github_branch",
            "pr_url", "pr_id_branch", "pr_source_branch",
            "build_id", "test_type_details", "test_run_status"
        )}
        # Should not raise
        storage.store_test_set_data([record])

        # Ensure at least one cursor.execute(...) was awaited
        self.assertTrue(fake_cursor.execute.await_count >= 1)

    # ───────────────── get_test_set_data() branches ─────────────────────────── #
    def test_get_test_set_data_artifact_success(self):
        """
        If get_test_set_data(...) finds rows, it should return a list of strings.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        fake_cursor.fetchall.return_value = [("json-1",), ("json-2",)]

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, val):
                self._val = val

            def result(self):
                return self._val

        @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
        def inner_test(m_run_tc):
            def fake_run(coro, loop):
                res = asyncio.get_event_loop().run_until_complete(coro)
                return DummyFuture(res)

            m_run_tc.side_effect = fake_run

            out = storage.get_test_set_data(artifact_id="A-1")
            self.assertEqual(out, ["json-1", "json-2"])

        inner_test()

    def test_get_test_set_data_not_found(self):
        """
        If no rows are returned, get_test_set_data(...) should raise RecordNotFoundException.
        """
        fake_pool, fake_cursor = _make_pool(closed=False)
        fake_cursor.fetchall.return_value = []

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, val):
                self._val = val

            def result(self):
                return self._val

        @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
        def inner_test(m_run_tc):
            def fake_run(coro, loop):
                try:
                    res = asyncio.get_event_loop().run_until_complete(coro)
                    return DummyFuture(res)
                except Exception as e:
                    return DummyFuture(e)

            m_run_tc.side_effect = fake_run

            with self.assertRaises(RecordNotFoundException):
                storage.get_test_set_data(test_request_id="T-1")

        inner_test()

    # ───────────────────────── stopped() path ──────────────────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_close(self, m_run_tc):
        """
        After initialization, calling close() should schedule _close_pool()
        and stop the loop.
        """
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        fake_pool, _ = _make_pool(closed=False)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()
        storage._thread = MagicMock()

        class DummyFuture:
            def __init__(self, val):
                self._val = val

            def result(self):
                return self._val

        def fake_run(coro, loop):
            res = asyncio.get_event_loop().run_until_complete(coro)
            return DummyFuture(res)

        m_run_tc.side_effect = fake_run

        # Call close() – should not raise
        storage.close()

        # Confirm that pool.close() was awaited
        fake_pool.close.assert_awaited_once()
        # Confirm that thread.join() was called
        storage._thread.join.assert_called()


if __name__ == "__main__":
    unittest.main()

