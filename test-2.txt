# File: lib/collectors/sevenps/storage/postgres.py

from __future__ import annotations
import os
import threading
import asyncio
from typing import List, Dict, Optional

from psycopg import AsyncConnectionPool
from psycopg.errors import UniqueViolation
from lib.collectors.sevenps.base_storage import SevenPSStorage, BaseStorageException, RecordNotFoundException
from lib.logging.logger import get_logger
from lib.cyber_logging import get_cyber_logger
from lib.utils import generate_db_conn_info
from cyber_schema_model import Schema20Event  # ← adjust import path if needed

logger = get_logger(__name__)
cyber_logger = get_cyber_logger()


class DatabaseConnectionError(BaseStorageException):
    """Cannot connect to DB"""


class DatabaseError(BaseStorageException):
    """General DB error"""


class PostgresDBStorage(SevenPSStorage):
    """
    A Postgres storage class whose public methods are all synchronous (or coroutine-compatible).
    Internally, it spins up one background thread + asyncio loop, creates an AsyncConnectionPool there,
    and schedules all DB work via asyncio.run_coroutine_threadsafe(…).
    """

    def __init__(self, config: Dict):
        """
        Synchronous constructor.  Spawns a background thread+loop, schedules _init_pool(conninfo)
        on that loop, and blocks until _init_pool either succeeds or raises.  If pool-creation fails,
        re-raise DatabaseConnectionError.
        """
        self.config = config
        self._loop: asyncio.AbstractEventLoop
        self._thread: threading.Thread
        self._pool: Optional[AsyncConnectionPool] = None

        # 1) Determine LOCAL vs CLOUD
        mode = os.getenv("CLOUD", "LOCAL").upper()
        is_cloud = (mode == "CLOUD")

        if is_cloud:
            conninfo = generate_db_conn_info()
        else:
            conninfo = (
                "host=localhost port=5432 dbname=testinginsightsdb "
                "user=user password=password"
            )

        # 2) Start a brand-new event loop in a daemon thread
        self._loop = asyncio.new_event_loop()
        self._thread = threading.Thread(
            target=self._start_background_loop,
            daemon=True,
            name="postgres-bg-loop",
        )
        self._thread.start()

        # 3) Schedule the actual pool-creation coroutine (_init_pool) on that loop.
        try:
            init_future = asyncio.run_coroutine_threadsafe(
                self._init_pool(conninfo), self._loop
            )
            init_future.result()  # if _init_pool raised, we get that exception here
            logger.debug("Successfully initialized AsyncConnectionPool")
        except Exception as exc:
            # Report a Schema20Event and then re-raise as DatabaseConnectionError
            event = Schema20Event(
                event_name="Unable to connect to DB",
                event_severity="ERROR"
            )
            cyber_logger.log(event)
            raise DatabaseConnectionError(
                "SEVENPS-COLLECTOR-ERROR: Unable to connect to DB"
            ) from exc

    def __del__(self):
        """
        If someone forgets to call close(), this ensures we tear down the background loop.
        """
        try:
            if hasattr(self, "_loop") and self._loop.is_running():
                fut = asyncio.run_coroutine_threadsafe(self._close_pool(), self._loop)
                fut.result()
                self._loop.call_soon_threadsafe(self._loop.stop)
                self._thread.join(timeout=1)
        except Exception:
            pass

    def _start_background_loop(self):
        """
        Target for the background thread.  Sets that thread's event loop
        and then run_forever() on it.
        """
        asyncio.set_event_loop(self._loop)
        self._loop.run_forever()

    async def _init_pool(self, conninfo: str) -> None:
        """
        Create & open the AsyncConnectionPool on self._loop.  Store it to self._pool.
        """
        min_c = self.config.get("min_connections", 1)
        max_c = self.config.get("max_connections", 5)
        self._pool = AsyncConnectionPool(
            conninfo=conninfo,
            min_size=min_c,
            max_size=max_c,
            timeout=180,
        )
        await self._pool.open()
        logger.debug("Connection pool opened on background loop")

    async def _ensure_pool_open_async(self) -> None:
        """
        Ensure that the pool exists and is open.  If closed, reopen it.
        """
        if self._pool is None:
            raise DatabaseConnectionError("Pool was never initialized")
        if self._pool.closed:
            await self._pool.open()
            logger.debug("Re-opened connection pool")

    async def _close_pool(self) -> None:
        """
        Coroutine to close the pool cleanly.
        """
        if self._pool and not self._pool.closed:
            await self._pool.close()
            logger.debug("Connection pool closed")

    def close(self) -> None:
        """
        Synchronous close(): schedule _close_pool on the background loop,
        wait for it to finish, then stop the loop and join the thread.
        """
        if not hasattr(self, "_loop"):
            return

        if self._pool is not None:
            fut = asyncio.run_coroutine_threadsafe(self._close_pool(), self._loop)
            fut.result()

        # Now stop the loop and join the thread
        self._loop.call_soon_threadsafe(self._loop.stop)
        self._thread.join(timeout=5)
        logger.debug("Background loop stopped, thread joined")

    #
    # ───────────────────────── Public “sync” methods ──────────────────────────
    #

    def _execute(self, query: str, params: Optional[Dict] = None) -> None:
        """
        Synchronous wrapper around _execute_async.  Blocks until done.
        """
        coro = self._execute_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def _execute_async(self, query: str, params: Optional[Dict] = None) -> None:
        """
        The real async coroutine that does cursor.execute + commit.
        """
        await self._ensure_pool_open_async()
        assert self._pool is not None
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    await conn.commit()
                    logger.info("Executed query: %s params: %s", query, params)
                except Exception as exc:
                    logger.error("Unable to execute query: %s", exc)
                    raise

    def fetch_all(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        Synchronous wrapper around fetch_all_async.
        """
        coro = self.fetch_all_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def fetch_all_async(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        The real async coroutine that does execute + fetchall.
        """
        await self._ensure_pool_open_async()
        assert self._pool is not None
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    rows = await cursor.fetchall()
                    return rows
                except Exception as exc:
                    logger.error("Unable to fetch from DB: %s", exc)
                    raise

    def _fetch_all(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        Provided so that tests which do `await storage._fetch_all(...)` continue to work.
        """
        return asyncio.get_event_loop().run_until_complete(self.fetch_all_async(query, params))

    def store_test_set_data(self, records: List[Dict]) -> None:
        """
        Synchronous wrapper around store_test_set_data_async.
        Check for empty list before scheduling any coroutine.
        """
        if not records:
            # Raise ValueError immediately, without ever touching the event loop
            raise ValueError("SEVENPS-COLLECTOR-ERROR: No records to store")

        coro = self.store_test_set_data_async(records)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def store_test_set_data_async(self, records: List[Dict]) -> None:
        """
        The real async coroutine: insert each record into sevenps_result_set with retries.
        """
        # (We know records is non-empty because the sync wrapper already checked.)
        tries = 6
        record_count = len(records)

        await self._ensure_pool_open_async()
        assert self._pool is not None

        while tries >= 0:
            try:
                async with self._pool.connection() as conn:
                    async with conn.cursor() as cursor:
                        for record in records:
                            logger.info(
                                "Inserting record – Artifact: %s:%s",
                                record.get("artifact_url"),
                                record.get("artifact_version"),
                            )
                            await cursor.execute(
                                """
                                INSERT INTO sevenps_result_set
                                  (repo_url, artifact_url, artifact_name, artifact_version,
                                   test_set_type, test_request_id, component_asv, component_bap,
                                   report_doc, report_source, traceability_doc,
                                   github_org, github_repo, github_branch,
                                   pr_url, pr_id_branch, pr_source_branch,
                                   build_id, test_type_details, test_run_status)
                                VALUES
                                  (%(repo_url)s, %(artifact_url)s, %(artifact_name)s, %(artifact_version)s,
                                   %(test_set_type)s, %(test_request_id)s, %(component_asv)s, %(component_bap)s,
                                   %(report_doc)s, %(report_source)s, %(traceability_doc)s,
                                   %(github_org)s, %(github_repo)s, %(github_branch)s,
                                   %(pr_url)s, %(pr_id_branch)s, %(pr_source_branch)s,
                                   %(build_id)s, %(test_type_details)s, %(test_run_status)s)
                                """,
                                {
                                    "repo_url": record.get("repo_url"),
                                    "artifact_url": record.get("artifact_url"),
                                    "artifact_name": record.get("artifact_name"),
                                    "artifact_version": record.get("artifact_version"),
                                    "test_set_type": record.get("test_set_type"),
                                    "test_request_id": record.get("test_request_id"),
                                    "component_asv": record.get("component_asv"),
                                    "component_bap": record.get("component_bap"),
                                    "report_doc": record.get("report_doc"),
                                    "report_source": record.get("report_source"),
                                    "traceability_doc": record.get("traceability_doc"),
                                    "github_org": record.get("github_org"),
                                    "github_repo": record.get("github_repo"),
                                    "github_branch": record.get("github_branch"),
                                    "pr_url": record.get("pr_url"),
                                    "pr_id_branch": record.get("pr_id_branch"),
                                    "pr_source_branch": record.get("pr_source_branch"),
                                    "build_id": record.get("build_id"),
                                    "test_type_details": record.get("test_type_details"),
                                    "test_run_status": record.get("test_run_status"),
                                },
                            )
                        await conn.commit()
                        logger.info(
                            "Successfully wrote %d records to sevenps_result_set", record_count
                        )
                        return
            except UniqueViolation as uv:
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Unique Violation when writing to DB :: %s",
                    str(uv),
                )
                raise
            except Exception as exc:
                tries -= 1
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Couldn't write record to DB, tries left: %d :: %s",
                    tries,
                    str(exc),
                )
                await asyncio.sleep(30)
                if tries < 0:
                    event = Schema20Event(
                        event_name="Unable to write to DB",
                        event_severity="ERROR",
                        event_outcome="FAILURE",
                    )
                    cyber_logger.log(event)
                    logger.error("SEVENPS-COLLECTOR-ERROR: Bad Record: %s", str(record))
                    raise DatabaseError(
                        "SEVENPS-COLLECTOR-ERROR: Unable to write record to db: " + str(exc)
                    ) from exc

    async def store_test_set_data_async(self, records: List[Dict]) -> None:
        """
        Provided so that tests doing `await storage.store_test_set_data_async(...)` will still work.
        """
        return await self.store_test_set_data_async(records)

    def get_test_set_data(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        Synchronous wrapper around get_test_set_data_async.
        """
        coro = self.get_test_set_data_async(test_request_id=test_request_id, artifact_id=artifact_id)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def get_test_set_data_async(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        The real async coroutine: SELECT results FROM sevenps_result_set WHERE [filter].
        Returns a list of JSON strings (first column of each row).
        """
        if artifact_id:
            sql = "SELECT results FROM sevenps_result_set WHERE artifact_id = $1"
            params = (artifact_id,)
        elif test_request_id:
            sql = "SELECT results FROM sevenps_result_set WHERE test_request_id = $1"
            params = (test_request_id,)
        else:
            raise ValueError("artifact_id or test_request_id is required")

        await self._ensure_pool_open_async()
        assert self._pool is not None

        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(sql, params)
                rows = await cursor.fetchall()

        if not rows:
            raise RecordNotFoundException("SEVENPS-COLLECTOR-ERROR: No test results found with the provided ID")

        return [r[0] for r in rows]
