# File: lib/collectors/sevenps/storage/postgres.py

from __future__ import annotations
import os
import threading
import asyncio
from typing import List, Dict, Optional

from psycopg import AsyncConnectionPool
from psycopg.errors import UniqueViolation
from lib.collectors.sevenps.base_storage import SevenPSStorage, BaseStorageException, RecordNotFoundException
from lib.logging.logger import get_logger
from lib.cyber_logging import get_cyber_logger
from lib.utils import generate_db_conn_info
from cyber_schema_model import Schema20Event  # ← adjust import path if needed

logger = get_logger(__name__)
cyber_logger = get_cyber_logger()


class DatabaseConnectionError(BaseStorageException):
    """Cannot connect to DB"""


class DatabaseError(BaseStorageException):
    """General DB error"""


class PostgresDBStorage(SevenPSStorage):
    """
    A Postgres storage class whose public methods are all synchronous (or coroutine-compatible).
    Internally, it spins up one background thread + asyncio loop, creates an AsyncConnectionPool there,
    and schedules all DB work via asyncio.run_coroutine_threadsafe(…).
    """

    def __init__(self, config: Dict):
        """
        Synchronous constructor.  Spawns a background thread+loop, schedules _init_pool(conninfo)
        on that loop, and blocks until _init_pool either succeeds or raises.  If pool-creation fails,
        re-raise DatabaseConnectionError.
        """
        self.config = config
        self._loop: asyncio.AbstractEventLoop
        self._thread: threading.Thread
        self._pool: Optional[AsyncConnectionPool] = None

        # 1) Determine LOCAL vs CLOUD
        mode = os.getenv("CLOUD", "LOCAL").upper()
        is_cloud = (mode == "CLOUD")

        if is_cloud:
            conninfo = generate_db_conn_info()
        else:
            conninfo = (
                "host=localhost port=5432 dbname=testinginsightsdb "
                "user=user password=password"
            )

        # 2) Start a brand-new event loop in a daemon thread
        self._loop = asyncio.new_event_loop()
        self._thread = threading.Thread(
            target=self._start_background_loop,
            daemon=True,
            name="postgres-bg-loop",
        )
        self._thread.start()

        # 3) Schedule the actual pool-creation coroutine (_init_pool) on that loop.
        try:
            init_future = asyncio.run_coroutine_threadsafe(
                self._init_pool(conninfo), self._loop
            )
            init_future.result()  # if _init_pool raised, we get that exception here
            logger.debug("Successfully initialized AsyncConnectionPool")
        except Exception as exc:
            # Report a Schema20Event and then re-raise as DatabaseConnectionError
            event = Schema20Event(
                event_name="Unable to connect to DB",
                event_severity="ERROR"
            )
            cyber_logger.log(event)
            raise DatabaseConnectionError(
                "SEVENPS-COLLECTOR-ERROR: Unable to connect to DB"
            ) from exc

    def __del__(self):
        """
        If someone forgets to call close(), this ensures we tear down the background loop.
        """
        try:
            if hasattr(self, "_loop") and self._loop.is_running():
                fut = asyncio.run_coroutine_threadsafe(self._close_pool(), self._loop)
                fut.result()
                self._loop.call_soon_threadsafe(self._loop.stop)
                self._thread.join(timeout=1)
        except Exception:
            pass

    def _start_background_loop(self):
        """
        Target for the background thread.  Sets that thread's event loop
        and then run_forever() on it.
        """
        asyncio.set_event_loop(self._loop)
        self._loop.run_forever()

    async def _init_pool(self, conninfo: str) -> None:
        """
        Create & open the AsyncConnectionPool on self._loop.  Store it to self._pool.
        """
        min_c = self.config.get("min_connections", 1)
        max_c = self.config.get("max_connections", 5)
        self._pool = AsyncConnectionPool(
            conninfo=conninfo,
            min_size=min_c,
            max_size=max_c,
            timeout=180,
        )
        await self._pool.open()
        logger.debug("Connection pool opened on background loop")

    async def _ensure_pool_open_async(self) -> None:
        """
        Ensure that the pool exists and is open.  If closed, reopen it.
        """
        if self._pool is None:
            raise DatabaseConnectionError("Pool was never initialized")
        if self._pool.closed:
            await self._pool.open()
            logger.debug("Re-opened connection pool")

    async def _close_pool(self) -> None:
        """
        Coroutine to close the pool cleanly.
        """
        if self._pool and not self._pool.closed:
            await self._pool.close()
            logger.debug("Connection pool closed")

    def close(self) -> None:
        """
        Synchronous close(): schedule _close_pool on the background loop,
        wait for it to finish, then stop the loop and join the thread.
        """
        if not hasattr(self, "_loop"):
            return

        if self._pool is not None:
            fut = asyncio.run_coroutine_threadsafe(self._close_pool(), self._loop)
            fut.result()

        # Now stop the loop and join the thread
        self._loop.call_soon_threadsafe(self._loop.stop)
        self._thread.join(timeout=5)
        logger.debug("Background loop stopped, thread joined")

    #
    # ───────────────────────── Public “sync” methods ──────────────────────────
    #

    def _execute(self, query: str, params: Optional[Dict] = None) -> None:
        """
        Synchronous wrapper around _execute_async.  Blocks until done.
        """
        coro = self._execute_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def _execute_async(self, query: str, params: Optional[Dict] = None) -> None:
        """
        The real async coroutine that does cursor.execute + commit.
        """
        await self._ensure_pool_open_async()
        assert self._pool is not None
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    await conn.commit()
                    logger.info("Executed query: %s params: %s", query, params)
                except Exception as exc:
                    logger.error("Unable to execute query: %s", exc)
                    raise

    def fetch_all(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        Synchronous wrapper around fetch_all_async.
        """
        coro = self.fetch_all_async(query, params)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def fetch_all_async(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        The real async coroutine that does execute + fetchall.
        """
        await self._ensure_pool_open_async()
        assert self._pool is not None
        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                try:
                    await cursor.execute(query, params)
                    rows = await cursor.fetchall()
                    return rows
                except Exception as exc:
                    logger.error("Unable to fetch from DB: %s", exc)
                    raise

    def _fetch_all(self, query: str, params: Optional[Dict] = None) -> List[tuple]:
        """
        Provided so that tests which do `await storage._fetch_all(...)` continue to work.
        """
        return asyncio.get_event_loop().run_until_complete(self.fetch_all_async(query, params))

    def store_test_set_data(self, records: List[Dict]) -> None:
        """
        Synchronous wrapper around store_test_set_data_async.
        Check for empty list before scheduling any coroutine.
        """
        if not records:
            # Raise ValueError immediately, without ever touching the event loop
            raise ValueError("SEVENPS-COLLECTOR-ERROR: No records to store")

        coro = self.store_test_set_data_async(records)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def store_test_set_data_async(self, records: List[Dict]) -> None:
        """
        The real async coroutine: insert each record into sevenps_result_set with retries.
        """
        # (We know records is non-empty because the sync wrapper already checked.)
        tries = 6
        record_count = len(records)

        await self._ensure_pool_open_async()
        assert self._pool is not None

        while tries >= 0:
            try:
                async with self._pool.connection() as conn:
                    async with conn.cursor() as cursor:
                        for record in records:
                            logger.info(
                                "Inserting record – Artifact: %s:%s",
                                record.get("artifact_url"),
                                record.get("artifact_version"),
                            )
                            await cursor.execute(
                                """
                                INSERT INTO sevenps_result_set
                                  (repo_url, artifact_url, artifact_name, artifact_version,
                                   test_set_type, test_request_id, component_asv, component_bap,
                                   report_doc, report_source, traceability_doc,
                                   github_org, github_repo, github_branch,
                                   pr_url, pr_id_branch, pr_source_branch,
                                   build_id, test_type_details, test_run_status)
                                VALUES
                                  (%(repo_url)s, %(artifact_url)s, %(artifact_name)s, %(artifact_version)s,
                                   %(test_set_type)s, %(test_request_id)s, %(component_asv)s, %(component_bap)s,
                                   %(report_doc)s, %(report_source)s, %(traceability_doc)s,
                                   %(github_org)s, %(github_repo)s, %(github_branch)s,
                                   %(pr_url)s, %(pr_id_branch)s, %(pr_source_branch)s,
                                   %(build_id)s, %(test_type_details)s, %(test_run_status)s)
                                """,
                                {
                                    "repo_url": record.get("repo_url"),
                                    "artifact_url": record.get("artifact_url"),
                                    "artifact_name": record.get("artifact_name"),
                                    "artifact_version": record.get("artifact_version"),
                                    "test_set_type": record.get("test_set_type"),
                                    "test_request_id": record.get("test_request_id"),
                                    "component_asv": record.get("component_asv"),
                                    "component_bap": record.get("component_bap"),
                                    "report_doc": record.get("report_doc"),
                                    "report_source": record.get("report_source"),
                                    "traceability_doc": record.get("traceability_doc"),
                                    "github_org": record.get("github_org"),
                                    "github_repo": record.get("github_repo"),
                                    "github_branch": record.get("github_branch"),
                                    "pr_url": record.get("pr_url"),
                                    "pr_id_branch": record.get("pr_id_branch"),
                                    "pr_source_branch": record.get("pr_source_branch"),
                                    "build_id": record.get("build_id"),
                                    "test_type_details": record.get("test_type_details"),
                                    "test_run_status": record.get("test_run_status"),
                                },
                            )
                        await conn.commit()
                        logger.info(
                            "Successfully wrote %d records to sevenps_result_set", record_count
                        )
                        return
            except UniqueViolation as uv:
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Unique Violation when writing to DB :: %s",
                    str(uv),
                )
                raise
            except Exception as exc:
                tries -= 1
                logger.error(
                    "SEVENPS-COLLECTOR-ERROR: Couldn't write record to DB, tries left: %d :: %s",
                    tries,
                    str(exc),
                )
                await asyncio.sleep(30)
                if tries < 0:
                    event = Schema20Event(
                        event_name="Unable to write to DB",
                        event_severity="ERROR",
                        event_outcome="FAILURE",
                    )
                    cyber_logger.log(event)
                    logger.error("SEVENPS-COLLECTOR-ERROR: Bad Record: %s", str(record))
                    raise DatabaseError(
                        "SEVENPS-COLLECTOR-ERROR: Unable to write record to db: " + str(exc)
                    ) from exc

    async def store_test_set_data_async(self, records: List[Dict]) -> None:
        """
        Provided so that tests doing `await storage.store_test_set_data_async(...)` will still work.
        """
        return await self.store_test_set_data_async(records)

    def get_test_set_data(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        Synchronous wrapper around get_test_set_data_async.
        """
        coro = self.get_test_set_data_async(test_request_id=test_request_id, artifact_id=artifact_id)
        fut = asyncio.run_coroutine_threadsafe(coro, self._loop)
        return fut.result()

    async def get_test_set_data_async(self, *, test_request_id: Optional[str] = None, artifact_id: Optional[str] = None) -> List[str]:
        """
        The real async coroutine: SELECT results FROM sevenps_result_set WHERE [filter].
        Returns a list of JSON strings (first column of each row).
        """
        if artifact_id:
            sql = "SELECT results FROM sevenps_result_set WHERE artifact_id = $1"
            params = (artifact_id,)
        elif test_request_id:
            sql = "SELECT results FROM sevenps_result_set WHERE test_request_id = $1"
            params = (test_request_id,)
        else:
            raise ValueError("artifact_id or test_request_id is required")

        await self._ensure_pool_open_async()
        assert self._pool is not None

        async with self._pool.connection() as conn:
            async with conn.cursor() as cursor:
                await cursor.execute(sql, params)
                rows = await cursor.fetchall()

        if not rows:
            raise RecordNotFoundException("SEVENPS-COLLECTOR-ERROR: No test results found with the provided ID")

        return [r[0] for r in rows]



-------------------------



# File: tests/test_db.py

import unittest
import asyncio
from unittest.mock import AsyncMock, patch, MagicMock

from psycopg import errors

from lib.collectors.sevenps.storage.postgres import (
    PostgresDBStorage,
    DatabaseConnectionError,
    DatabaseError,
    RecordNotFoundException,
)

PATCH_ROOT = "lib.collectors.sevenps.storage.postgres"


def _make_fake_pool(closed: bool = True):
    """
    Build (fake_pool, fake_cursor) where:
      - fake_pool is an AsyncMock that has a .connection() returning an async‐CM that yields fake_conn
      - fake_conn is an AsyncMock whose .cursor() returns an async‐CM yielding fake_cursor
      - fake_cursor is an AsyncMock with .execute() and .fetchall() as AsyncMock
    """
    fake_pool = AsyncMock()
    fake_pool.closed = closed
    fake_pool.open = AsyncMock()
    fake_pool.close = AsyncMock()

    fake_conn = AsyncMock()
    fake_conn.commit = AsyncMock()

    fake_cursor = AsyncMock()
    fake_cursor.execute = AsyncMock()
    fake_cursor.fetchall = AsyncMock()

    # Build "async with pool.connection() as conn" → yields fake_conn
    conn_cm = AsyncMock()
    conn_cm.__aenter__.return_value = fake_conn
    conn_cm.__aexit__.return_value = False
    fake_pool.connection.return_value = conn_cm

    # Build "async with conn.cursor() as cursor" → yields fake_cursor
    cur_cm = AsyncMock()
    cur_cm.__aenter__.return_value = fake_cursor
    cur_cm.__aexit__.return_value = False
    fake_conn.cursor.return_value = cur_cm

    return fake_pool, fake_cursor


class TestPostgresDBStorage(unittest.TestCase):
    # ───────────────────────── constructor paths ────────────────────────────── #
    @patch(f"{PATCH_ROOT}.AsyncConnectionPool")
    @patch(f"{PATCH_ROOT}.os.getenv", return_value="LOCAL")
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_init_local_success(self, m_run_tc, m_env, m_pool_cls):
        """
        If os.getenv('CLOUD') → 'LOCAL', then __init__ should do:
         - AsyncConnectionPool(...) exactly once
         - schedule run_coroutine_threadsafe(_init_pool) once
        """
        # 1) Fake pool = AsyncMock() that the constructor will receive.
        fake_pool, _ = _make_fake_pool(closed=True)
        m_pool_cls.return_value = fake_pool

        # 2) Make run_coroutine_threadsafe(...) return a dummy future whose .result() does nothing
        class DummyFuture:
            def result(self):
                return None

        m_run_tc.return_value = DummyFuture()

        # 3) Now call constructor – should not raise
        PostgresDBStorage({"min_connections": 1, "max_connections": 2})

        # 4) Ensure AsyncConnectionPool() was invoked exactly once
        m_pool_cls.assert_called_once()
        # 5) Ensure run_coroutine_threadsafe was also called (once)
        self.assertTrue(m_run_tc.called)

    @patch(f"{PATCH_ROOT}.AsyncConnectionPool", side_effect=Exception("boom"))
    @patch(f"{PATCH_ROOT}.os.getenv", return_value="CLOUD")
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_init_cloud_failure(self, m_run_tc, m_env, m_pool_cls):
        """
        If os.getenv('CLOUD') → 'CLOUD' and AsyncConnectionPool(...) raises,
        then __init__ should re‐raise DatabaseConnectionError.
        """
        with self.assertRaises(DatabaseConnectionError):
            PostgresDBStorage({})

    # ─────────────── ensure_pool_open_async() + _close_pool() behavior ─────────────────── #
    def test_pool_open_and_close_coroutines(self):
        """
        Directly test the async coroutines:
          - If pool.closed=True, then await pool.open(), else not.
          - Then test that _close_pool() awaits pool.close().
        """
        fake_pool, _ = _make_fake_pool(closed=True)
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool

        # Bind _ensure_pool_open_async and _close_pool onto storage
        ensure_coro = PostgresDBStorage._ensure_pool_open_async.__get__(storage)
        close_coro = PostgresDBStorage._close_pool.__get__(storage)

        # Step 1: pool.closed=True → open() should be awaited once
        loop = asyncio.new_event_loop()
        try:
            loop.run_until_complete(ensure_coro())
            fake_pool.open.assert_awaited_once()
        finally:
            loop.close()

        # Step 2: pool.closed=False → open() not awaited again
        fake_pool.open.reset_mock()
        fake_pool.closed = False
        loop = asyncio.new_event_loop()
        try:
            loop.run_until_complete(ensure_coro())
            fake_pool.open.assert_not_awaited()
        finally:
            loop.close()

        # Step 3: _close_pool() should await pool.close()
        loop = asyncio.new_event_loop()
        try:
            loop.run_until_complete(close_coro())
            fake_pool.close.assert_awaited_once()
        finally:
            loop.close()

    # ───────────────────────── _execute() paths ─────────────────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_execute_success(self, m_run_tc):
        """
        Test that _execute(...) calls cursor.execute + conn.commit() under the hood.
        """
        fake_pool, fake_cursor = _make_fake_pool(closed=False)

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()  # unused because run_coroutine_threadsafe is patched

        # We want run_coroutine_threadsafe(...) to run the _execute_async coroutine
        # on a fresh loop, then return a DummyFuture(None).
        class DummyFuture:
            def __init__(self, val):
                self._val = val
            def result(self):
                return self._val

        def fake_run(coro, loop):
            # Run the coroutine on a fresh loop:
            runner = asyncio.new_event_loop()
            try:
                res = runner.run_until_complete(coro)
            finally:
                runner.close()
            return DummyFuture(res)

        m_run_tc.side_effect = fake_run

        # Now call the sync wrapper
        storage._execute("INSERT INTO xyz(id) VALUES(%s)", {"id": 1})

        # Inside _execute_async, it should have done: await cursor.execute(...); await conn.commit()
        fake_cursor.execute.assert_awaited_once_with("INSERT INTO xyz(id) VALUES(%s)", {"id": 1})
        fake_pool.connection.return_value.__aenter__.return_value.commit.assert_awaited_once()

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_execute_failure(self, m_run_tc):
        """
        If cursor.execute(...) raises inside _execute_async, then _execute(...) should re-raise.
        """
        fake_pool, fake_cursor = _make_fake_pool(closed=False)
        fake_cursor.execute.side_effect = Exception("boom")

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, result_or_exc):
                self._res = result_or_exc
            def result(self):
                if isinstance(self._res, Exception):
                    raise self._res
                return self._res

        def fake_run(coro, loop):
            runner = asyncio.new_event_loop()
            try:
                res = runner.run_until_complete(coro)
                return DummyFuture(res)
            except Exception as e:
                return DummyFuture(e)
            finally:
                runner.close()

        m_run_tc.side_effect = fake_run

        with self.assertRaises(Exception):
            storage._execute("INSERT BAD", None)

    # ───────────────────────── _fetch_all() path ───────────────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_fetch_all(self, m_run_tc):
        """
        Test that fetch_all(...) returns whatever fetch_all_async returned.
        """
        fake_pool, fake_cursor = _make_fake_pool(closed=False)
        fake_cursor.fetchall.return_value = [(1,), (2,)]

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, val):
                self._val = val
            def result(self):
                return self._val

        def fake_run(coro, loop):
            runner = asyncio.new_event_loop()
            try:
                res = runner.run_until_complete(coro)
                return DummyFuture(res)
            finally:
                runner.close()

        m_run_tc.side_effect = fake_run

        rows = storage.fetch_all("SELECT * FROM foo", None)
        self.assertEqual(rows, [(1,), (2,)])

    # ───────────── store_test_set_data() edge + happy paths ────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_store_test_set_data_value_error(self, m_run_tc):
        """
        If records == [], synchronous wrapper must immediately raise ValueError.
        """
        fake_pool, _ = _make_fake_pool(closed=False)
        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        with self.assertRaises(ValueError):
            storage.store_test_set_data([])  # no coroutine should be scheduled at all

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_store_test_set_data_success(self, m_run_tc):
        """
        If you supply a well-formed record, no exception should be raised,
        and cursor.execute(...) should be awaited at least once.
        """
        fake_pool, fake_cursor = _make_fake_pool(closed=False)

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, val):
                self._val = val
            def result(self):
                return self._val

        def fake_run(coro, loop):
            runner = asyncio.new_event_loop()
            try:
                res = runner.run_until_complete(coro)
                return DummyFuture(res)
            finally:
                runner.close()

        m_run_tc.side_effect = fake_run

        # Build one dummy record with every required key
        record = {k: "x" for k in (
            "repo_url", "artifact_url", "artifact_name", "artifact_version",
            "test_set_type", "test_request_id", "component_asv", "component_bap",
            "report_doc", "report_source", "traceability_doc",
            "github_org", "github_repo", "github_branch",
            "pr_url", "pr_id_branch", "pr_source_branch",
            "build_id", "test_type_details", "test_run_status"
        )}

        # Should not raise
        storage.store_test_set_data([record])

        # Ensure we actually called cursor.execute at least once
        self.assertTrue(fake_cursor.execute.await_count >= 1)

    # ───────────────── get_test_set_data() branches ─────────────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_get_test_set_data_artifact_success(self, m_run_tc):
        """
        If get_test_set_data(...) finds rows, it should return a list of strings.
        """
        fake_pool, fake_cursor = _make_fake_pool(closed=False)
        fake_cursor.fetchall.return_value = [("json-1",), ("json-2",)]

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        class DummyFuture:
            def __init__(self, val):
                self._val = val
            def result(self):
                return self._val

        def fake_run(coro, loop):
            runner = asyncio.new_event_loop()
            try:
                res = runner.run_until_complete(coro)
                return DummyFuture(res)
            finally:
                runner.close()

        m_run_tc.side_effect = fake_run

        out = storage.get_test_set_data(artifact_id="A-1")
        self.assertEqual(out, ["json-1", "json-2"])

    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_get_test_set_data_not_found(self, m_run_tc):
        """
        If no rows are returned, get_test_set_data(...) should raise RecordNotFoundException.
        """
        fake_pool, fake_cursor = _make_fake_pool(closed=False)
        fake_cursor.fetchall.return_value = []

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()

        def fake_run(coro, loop):
            runner = asyncio.new_event_loop()
            try:
                return runner.run_until_complete(coro)
            finally:
                runner.close()

        class DummyFuture:
            def __init__(self, val_or_exc):
                self._val = val_or_exc
            def result(self):
                if isinstance(self._val, Exception):
                    raise self._val
                return self._val

        def fake_run_capture(coro, loop):
            runner = asyncio.new_event_loop()
            try:
                res = runner.run_until_complete(coro)
                return DummyFuture(res)
            except Exception as e:
                return DummyFuture(e)
            finally:
                runner.close()

        m_run_tc.side_effect = fake_run_capture

        with self.assertRaises(RecordNotFoundException):
            storage.get_test_set_data(test_request_id="T-1")

    # ───────────────────────── stopped() path ──────────────────────────────── #
    @patch(f"{PATCH_ROOT}.asyncio.run_coroutine_threadsafe")
    def test_close(self, m_run_tc):
        """
        After initialization, calling close() should schedule _close_pool()
        and stop the loop.
        """
        fake_pool, _ = _make_fake_pool(closed=False)

        storage = PostgresDBStorage.__new__(PostgresDBStorage)
        storage._pool = fake_pool
        storage._loop = asyncio.new_event_loop()
        storage._thread = MagicMock()

        class DummyFuture:
            def __init__(self, val):
                self._val = val
            def result(self):
                return self._val

        def fake_run(coro, loop):
            runner = asyncio.new_event_loop()
            try:
                res = runner.run_until_complete(coro)
                return DummyFuture(res)
            finally:
                runner.close()

        m_run_tc.side_effect = fake_run

        # Call close() – should not raise
        storage.close()

        # Confirm that pool.close() was awaited
        fake_pool.close.assert_awaited_once()
        # Confirm that thread.join() was called
        storage._thread.join.assert_called()

if __name__ == "__main__":
    unittest.main()
